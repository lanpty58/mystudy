{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ZzJrMEb0Ge2BKnYa3cKcFmRXFGqRJ2UB",
      "authorship_tag": "ABX9TyN7yASHiEA3E7ZkWXoNYWI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanpty58/mystudy/blob/main/DuckNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohbIem2LVAs-",
        "outputId": "24983405-0a63-4d71-9875-79f6f6675a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.1-py2.py3-none-any.whl (756 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.7.1\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy==1.21.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM-WIeU3_3oE",
        "outputId": "d56737c2-b544-4dd7-edf9-85801f44bf30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.21.2 in /usr/local/lib/python3.10/dist-packages (1.21.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy --upgrade --ignore-installed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "YAH48KsPIpwJ",
        "outputId": "0a735241-d6b9-4a39-cb1d-0337783a1ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.21.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "23ce3cc3641840178053d012b0dc0b14"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U opencv-python==4.8.0.76"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znJm1letGwYb",
        "outputId": "ae3dc9ab-39d1-4bc0-a629-6bbe255567cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python==4.8.0.76 in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.8.0.76) (1.21.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import albumentations as albu\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "from keras.callbacks import CSVLogger\n",
        "from datetime import datetime\n",
        "import os\n",
        "import cv2\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, precision_score, recall_score\n",
        "from kornia.losses import focal_loss"
      ],
      "metadata": {
        "id": "v8BYGaWDPQDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoN-EhSNX6Mg",
        "outputId": "4737b2aa-4473-4e3b-a4f5-d203cbd48559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 448\n",
        "dataset_type = 'kvasir'\n",
        "lr = 1e-4\n",
        "seed_value = 58800\n",
        "filters = 17\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "ct = datetime.now()\n",
        "model_type = 'DuckNet'\n",
        "progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
        "progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
        "plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
        "model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
        "\n",
        "EPOCHS = 600\n",
        "min_loss_for_saving = 0.2"
      ],
      "metadata": {
        "id": "-FtPfG_te5Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/AI/data/train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sLRVPyUglXX",
        "outputId": "fe384e10-47a5-4a27-fde5-7c3e925b76eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/AI/data/train.zip\n",
            "replace content/data/train/malignant/malignant (98)_mask.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Path\n",
        "OUTPUT_DIR = '/content/output'\n",
        "DATASET_DIR = '/content/content/data/train'\n",
        "CHECKPOINT = None\n",
        "\n",
        "#Eval\n",
        "WEIGHT = r''"
      ],
      "metadata": {
        "id": "B6KOY_4wHxhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_path(task):\n",
        "    weight_dir = os.path.join(OUTPUT_DIR, task)\n",
        "    os.makedirs(weight_dir, exist_ok=True)\n",
        "    log_dir = weight_dir\n",
        "    logger_name = f'{task}'\n",
        "    return weight_dir, log_dir, logger_name\n",
        "\n",
        "def setup_logger(logger_name, output_dir):\n",
        "    import os\n",
        "    logger = logging.getLogger(logger_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    # create file handler which logs even debug messages\n",
        "    fh = logging.FileHandler(os.path.join(output_dir, 'log.log'))\n",
        "    fh.setLevel(logging.DEBUG)\n",
        "    # create console handler with a higher log level\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    # create formatter and add it to the handlers\n",
        "    formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s: %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    # add the handlers to logger\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "    return logger"
      ],
      "metadata": {
        "id": "4dl1edOXH5ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output\n"
      ],
      "metadata": {
        "id": "Y6VIqBEUJSO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8491c5a-4b14-425b-a091-860e2c9ff2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7iJkWkIxPYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Conv2D, UpSampling2D, add\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "\n",
        "kernel_initializer = 'he_uniform'\n",
        "interpolation = 'nearest'\n",
        "def double_convolution_with_batch_normalization(x, filters, dilation_rate = 1):\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer= kernel_initializer, padding = 'same', dilation_rate = dilation_rate)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer= kernel_initializer, padding = 'same', dialation_rate = dilation_rate)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  return x\n",
        "\n",
        "def resnet_conv2D_block(x, filters, dilation_rate = 1):\n",
        "  x1 = Conv2D(filters, (1, 1), activation='relu', kernel_initializer= kernel_initializer, padding = 'same', dilation_rate = dilation_rate)(x)\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer=kernel_initializer, padding = 'same', dilation_rate = dilation_rate)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer= kernel_initializer, padding = 'same', dilation_rate = dilation_rate)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x_final = add([x, x1])\n",
        "  x_final = BatchNormalization(axis=-1)(x_final)\n",
        "  return x_final\n",
        "\n",
        "def widescope_conv2D_block(x, filters):\n",
        "  x = Conv2D(filters, (3, 3), activation ='relu', kernel_initializer= kernel_initializer, padding = 'same', dilation_rate = 1)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer= kernel_initializer, padding = 'same', dilation_rate = 2)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding = 'same', dilation_rate = 3)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  return x\n",
        "\n",
        "def midscope_conv2D_block(x, filters):\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer = kernel_initializer, padding = 'same', dilation_rate = 1)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x = Conv2D(filters, (3, 3), activation = 'relu', kernel_initializer = kernel_initializer, padding = 'same', dilation_rate = 2)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  return x\n",
        "\n",
        "def seperated_conv2D_block(x, filters, size = 3, padding = 'same'):\n",
        "  x = Conv2D(filters,(1, size), activation = 'relu', kernel_initializer = kernel_initializer, padding = padding)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  x = Conv2D(filters, (size, 1), activation = 'relu', kernel_initializer = kernel_initializer, padding = padding)(x)\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  return x\n",
        "\n",
        "def conv_block_2D(x, filters, block_type, repeat = 1, dilation_rate = 1, size = 3, padding = 'same'):\n",
        "  result = x\n",
        "  for i in range(0, repeat):\n",
        "    if block_type == 'separated':\n",
        "      result = seperated_conv2D_block(result, filters, size = size, padding = padding, )\n",
        "    elif block_type == 'duckv2':\n",
        "      result = duckv2_conv2D_block(result, filters, size = size)\n",
        "    elif block_type == 'midscope':\n",
        "      result = midscope_conv2D_block(result, filters)\n",
        "    elif block_type == 'widescope':\n",
        "      result = widescope_conv2D_block(result, filters)\n",
        "    elif block_type == 'resnet':\n",
        "      result = resnet_conv2D_block(result, filters, dilation_rate)\n",
        "    elif block_type == 'conv':\n",
        "      result = Conv2D(filters, (size, size),\n",
        "                      activation='relu', kernel_initializer=kernel_initializer, padding=padding)(result)\n",
        "    elif block_type == 'double_convolution':\n",
        "      result = double_convolution_with_batch_normalization(result, filters, dilation_rate)\n",
        "    else:\n",
        "      return None\n",
        "    return result\n",
        "\n",
        "def duckv2_conv2D_block(x, filters, size):\n",
        "  x = BatchNormalization(axis= -1)(x)\n",
        "  x1 = widescope_conv2D_block(x, filters)\n",
        "  x2 = midscope_conv2D_block(x, filters)\n",
        "  x3 = conv_block_2D(x, filters, 'resnet', repeat = 1)\n",
        "  x4 = conv_block_2D(x, filters, 'resnet', repeat = 2)\n",
        "  x5 = conv_block_2D(x, filters, 'resnet', repeat = 3)\n",
        "  x6 = seperated_conv2D_block(x, filters, size = 6, padding = 'same')\n",
        "  x = add([x1, x2, x3, x4, x5, x6])\n",
        "  x = BatchNormalization(axis = -1)(x)\n",
        "  return x\n",
        "\n",
        "def create_model(img_height, img_width, input_channels, out_classes, starting_filters):\n",
        "  input_layer = tf.keras.layers.Input((img_height, img_width, input_channels))\n",
        "  print( 'Starting DUCK-Net')\n",
        "  p1 = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(input_layer)\n",
        "  p2 = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(p1)\n",
        "  p3 = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(p2)\n",
        "  p4 = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(p3)\n",
        "  p5 = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(p4)\n",
        "  t0 = conv_block_2D(input_layer, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "  l1i = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(t0)\n",
        "  s1 = add([l1i, p1])\n",
        "  t1 = conv_block_2D(s1, starting_filters * 2, 'duckv2', repeat=1)\n",
        "  l2i = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(t1)\n",
        "  s2 = add([l2i, p2])\n",
        "  t2 = conv_block_2D(s2, starting_filters * 4, 'duckv2', repeat=1)\n",
        "  l3i = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(t2)\n",
        "  s3 = add([l3i, p3])\n",
        "  t3 = conv_block_2D(s3, starting_filters * 8, 'duckv2', repeat=1)\n",
        "\n",
        "  l4i = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(t3)\n",
        "  s4 = add([l4i, p4])\n",
        "  t4 = conv_block_2D(s4, starting_filters * 16, 'duckv2', repeat=1)\n",
        "\n",
        "  l5i = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(t4)\n",
        "  s5 = add([l5i, p5])\n",
        "  t51 = conv_block_2D(s5, starting_filters * 32, 'resnet', repeat=2)\n",
        "  t53 = conv_block_2D(t51, starting_filters * 16, 'resnet', repeat=2)\n",
        "\n",
        "  l5o = UpSampling2D((2, 2), interpolation=interpolation)(t53)\n",
        "  c4 = add([l5o, t4])\n",
        "  q4 = conv_block_2D(c4, starting_filters * 8, 'duckv2', repeat=1)\n",
        "\n",
        "  l4o = UpSampling2D((2, 2), interpolation=interpolation)(q4)\n",
        "  c3 = add([l4o, t3])\n",
        "  q3 = conv_block_2D(c3, starting_filters * 4, 'duckv2', repeat=1)\n",
        "  l3o = UpSampling2D((2, 2), interpolation=interpolation)(q3)\n",
        "  c2 = add([l3o, t2])\n",
        "  q6 = conv_block_2D(c2, starting_filters * 2, 'duckv2', repeat=1)\n",
        "\n",
        "  l2o = UpSampling2D((2, 2), interpolation=interpolation)(q6)\n",
        "  c1 = add([l2o, t1])\n",
        "  q1 = conv_block_2D(c1, starting_filters, 'duckv2', repeat=1)\n",
        "  l1o = UpSampling2D((2, 2), interpolation=interpolation)(q1)\n",
        "  c0 = add([l1o, t0])\n",
        "  z1 = conv_block_2D(c0, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "  output = Conv2D(out_classes, (1, 1), activation='sigmoid')(z1)\n",
        "\n",
        "  model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "JlgJ98aziLPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vBbLBqBwQFSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def dice_metric_loss(ground_truth, predictions, smooth = 1e-6):\n",
        "  ground_truth = K.cast(ground_truth, tf.float32)\n",
        "  predictions = K.cast(predictions, tf.float32)\n",
        "  ground_truth = K.flatten(ground_truth)\n",
        "  predictions = K.flatten(predictions)\n",
        "  intersection = K.sum(predictions*ground_truth)\n",
        "  union = K.sum(predictions) +K.sum(ground_truth)\n",
        "  dice = (2*intersection +smooth)/(union +smooth)\n",
        "  return dice"
      ],
      "metadata": {
        "id": "AlZ4e43snCDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(img_height=img_size, img_width = img_size, input_channels = 3, out_classes = 1, starting_filters = filters)\n",
        "model.compile(optimizer = optimizer, loss = f1_score)\n"
      ],
      "metadata": {
        "id": "m8bBWR7zf6Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3e2e43-4502-4d4b-a84a-f33e966c8442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DUCK-Net\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "def calculate_overlap_metrics(pred, gt, eps = 1e-5):\n",
        "\n",
        "  output = pred.view(-1,)\n",
        "  target = gt.view(-1,).float()\n",
        "\n",
        "  tp = K.sum(output * target)  # TP\n",
        "  fp = K.sum(output * (1 - target))  # FP\n",
        "  fn = K.sum((1 - output) * target)  # FN\n",
        "  tn = K.sum((1 - output) * (1 - target))  # TN\n",
        "\n",
        "  # pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n",
        "  dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)\n",
        "  iou = ( tp + eps) / ( tp + fp + fn + eps)\n",
        "  precision = (tp + eps) / (tp + fp + eps)\n",
        "  recall = (tp + eps) / (tp + fn + eps)\n",
        "  #specificity = (tn + eps) / (tn + fp + eps)\n",
        "\n",
        "  return iou, dice, precision, recall"
      ],
      "metadata": {
        "id": "Zny96B--GXeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6G-u5V2pEMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "from tqdm import tqdm\n",
        "def split_dataset(dataset_dir):\n",
        "  benign, malignant, normal = [], [], []\n",
        "  benign_images = [os.path.join(dataset_dir, 'benign', file) for file in os.listdir(os.path.join(dataset_dir, 'benign')) if file.endswith('.png')]\n",
        "  malignant_images = [os.path.join(dataset_dir, 'malignant', file) for file in os.listdir(os.path.join(dataset_dir, 'malignant')) if file.endswith('png')]\n",
        "  normal_images = [os.path.join(dataset_dir, 'normal', file) for file in os.listdir(os.path.join(dataset_dir, 'normal')) if file.endswith('.png')]\n",
        "\n",
        "  images, masks = [], []\n",
        "  image_paths = benign_images + malignant_images + normal_images\n",
        "  dataset_size = len(image_paths)\n",
        "  X = np.zeros((dataset_size, img_size, img_size, 3), dtype=np.float32)\n",
        "  Y = np.zeros((dataset_size, img_size, img_size), dtype=np.float32)\n",
        "\n",
        "  for i in range(len(image_paths)):\n",
        "        if '_mask' in image_paths[i]:\n",
        "          image_path = image_paths[i].replace('_mask.png', '.png')\n",
        "          # image = imread(image_path)\n",
        "          # mask = imread(image_paths[i])\n",
        "\n",
        "          # pillow_image = Image.fromarray(image)\n",
        "          # pillow_image = pillow_image.resize((img_size, img_size))\n",
        "          # image = np.array(pillow_image)\n",
        "\n",
        "          image = Image.open(image_path)\n",
        "          image = image.resize((img_size, img_size))\n",
        "          mask = Image.open(image_paths[i]).convert('L')\n",
        "          mask = mask.resize((img_size, img_size))\n",
        "\n",
        "          # Convert mask to NumPy array and initialize bi_mask\n",
        "          mask = np.array(mask)\n",
        "          bi_mask = np.zeros_like(mask)\n",
        "\n",
        "          # pillow_mask = Image.fromarray(mask)\n",
        "          # pillow_mask = pillow_mask.resize((img_size, img_size), resample=Image.LANCZOS)\n",
        "\n",
        "          # mask = np.array(pillow_mask)\n",
        "\n",
        "          # bi_mask = np.zeros((img_size, img_size), dtype=np.bool_)\n",
        "          # for r in range(img_size):\n",
        "          #   for c in range(img_size):\n",
        "          #       if mask[r, c] >= 127:\n",
        "          #           bi_mask[r, c] = 1\n",
        "          bi_mask = np.where(mask < 127, bi_mask, 1)\n",
        "          Y[i] = np.array(bi_mask)\n",
        "          X[i] = np.array(image)/255\n",
        "\n",
        "          # images.append(image)\n",
        "          # masks.append(mask)\n",
        "\n",
        "  Y = np.expand_dims(Y, axis=-1)\n",
        "\n",
        "  train_image, val_image, train_mask, val_mask = train_test_split(X, Y, test_size = 0.1, shuffle= True, random_state = 42)\n",
        "  return train_image, val_image, train_mask, val_mask\n"
      ],
      "metadata": {
        "id": "Rxz_Rf2PyFIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, val_images, train_masks, val_masks = split_dataset(DATASET_DIR)\n"
      ],
      "metadata": {
        "id": "SJ0JAw5Hpnmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 0\n",
        "N_FOLDS = 1\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "def train(train_images, val_images, train_masks, val_masks):\n",
        "  TASK = 'Segmentation'\n",
        "  weight_dir, log_dir, logger_name = init_path(TASK)\n",
        "  logger = setup_logger(logger_name, log_dir)\n",
        "  best_f1 = 0\n",
        "  stale = 0\n",
        "  start_epoch = 1\n",
        "\n",
        "  for epoch in range(start_epoch, EPOCHS+1):\n",
        "    train_images, train_masks = augment_images(train_images, train_masks)\n",
        "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
        "    model.fit(x = train_images, y = train_masks, epochs = 1, batch_size=BATCH_SIZE, validation_data=(val_images, val_masks), verbose=1, callbacks=[csv_logger])\n",
        "    prediction_valid = model.predict(val_images, verbose = 0)\n",
        "    loss_valid = dice_metric_loss(val_masks, prediction_valid)\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \"+ str(loss_valid))\n",
        "    with open(progress_path, 'a') as f:\n",
        "      f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\n\\n\\n')\n",
        "\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "      min_loss_for_saving = loss_valid\n",
        "      print('Saved model with val_loss: ', loss_valid)\n",
        "      model.save(model_path)\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "i1YAmjsKohzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNQY2soOEYI5",
        "outputId": "dbbd89cd-cf95-481d-e4ee-3c54f499f8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1121, 448, 448, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import albumentations as albu\n",
        "aug_train = albu.Compose([\n",
        "                  albu.HorizontalFlip(),\n",
        "                  albu.VerticalFlip(),\n",
        "                  albu.ColorJitter(brightness=(0.6, 1.6),contrast=0.2, saturation = 0.1, hue = 0.01, always_apply=True),\n",
        "                  albu.Affine(scale = (0.5, 1.5), translate_percent=(-0.125, 0.125), rotate = (-180, 180), shear = (-22.5, 22), always_apply=True)])\n",
        "\n",
        "def augment_images(train_images, train_masks):\n",
        "    x_train_out = []\n",
        "    y_train_out = []\n",
        "    # train_images = train_images.astype(np.uint8)\n",
        "    # train_masks = train_masks.astype(np.uint8)\n",
        "\n",
        "\n",
        "    for i in range (len(train_images)):\n",
        "\n",
        "\n",
        "        ug = aug_train(image=train_images[i], mask=train_masks[i])\n",
        "        x_train_out.append(ug['image'])\n",
        "        y_train_out.append(ug['mask'])\n",
        "\n",
        "    return np.array(x_train_out), np.array(y_train_out)"
      ],
      "metadata": {
        "id": "bC4Jk7lcxD-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDyrexQ7uhTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_images, val_images, train_masks, val_masks)"
      ],
      "metadata": {
        "id": "hmxM1SxQoF8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading the model\")\n",
        "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
        "prediction_train = model.predict(train_images, batch_size=BATCH_SIZE)\n",
        "prediction_valid = model.predict(val_images, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(\"Predictions done\")\n",
        "\n",
        "dice_train = f1_score(np.ndarray.flatten(np.array(train_masks, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "dice_valid = f1_score(np.ndarray.flatten(np.array(val_masks, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Dice finished\")\n",
        "\n",
        "\n",
        "miou_train = jaccard_score(np.ndarray.flatten(np.array(train_masks, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "\n",
        "miou_valid = jaccard_score(np.ndarray.flatten(np.array(val_masks, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Miou finished\")\n",
        "\n",
        "\n",
        "precision_train = precision_score(np.ndarray.flatten(np.array(train_masks, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
        "\n",
        "precision_valid = precision_score(np.ndarray.flatten(np.array(val_masks, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Precision finished\")\n",
        "\n",
        "\n",
        "recall_train = recall_score(np.ndarray.flatten(np.array(train_masks, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_train > 0.5))\n",
        "\n",
        "recall_valid = recall_score(np.ndarray.flatten(np.array(val_masks, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Recall finished\")\n",
        "\n",
        "\n",
        "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(train_masks, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_train > 0.5))\n",
        "\n",
        "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(val_masks, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "\n",
        "print(\"Accuracy finished\")\n",
        "\n",
        "\n",
        "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
        "print(final_file)\n",
        "\n",
        "with open(final_file, 'a') as f:\n",
        "    f.write(dataset_type + '\\n\\n')\n",
        "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + '\\n\\n')\n",
        "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + '\\n\\n')\n",
        "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid)  + '\\n\\n')\n",
        "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid)  + '\\n\\n')\n",
        "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid)  + '\\n\\n\\n\\n')\n",
        "\n",
        "print('File done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "B0UHbzhsXdfW",
        "outputId": "014af7b9-6e3b-449a-dc15-255775228a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at ModelSaveTensorFlow/kvasir/DuckNet_filters_17_2024-02-22 15:20:13.630923",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-af80d7658bff>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dice_metric_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdice_metric_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprediction_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprediction_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at ModelSaveTensorFlow/kvasir/DuckNet_filters_17_2024-02-22 15:20:13.630923"
          ]
        }
      ]
    }
  ]
}